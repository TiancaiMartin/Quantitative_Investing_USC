{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c77074eca94dea6",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2a24e313665a6",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "### 1. Dongtong Zhong (8124193969)\n",
    "### 2. Liwen Dai (5282656931)\n",
    "### 3. Feifan Gu (8135699631)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ca8aff804753a",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a7455a3de555b",
   "metadata": {},
   "source": [
    "### Question 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84459c23e294740d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:24.772406Z",
     "start_time": "2025-11-22T06:04:13.723595Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\97909\\AppData\\Local\\Temp\\ipykernel_9852\\3673629370.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  crsp = pd.read_csv(\"CRSP_Indexes.csv\", parse_dates=['DATE'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LV</th>\n",
       "      <th>LG</th>\n",
       "      <th>SV</th>\n",
       "      <th>SG</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>2050.79</td>\n",
       "      <td>2517.44</td>\n",
       "      <td>1955.12</td>\n",
       "      <td>2218.42</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>2035.21</td>\n",
       "      <td>2502.53</td>\n",
       "      <td>1948.33</td>\n",
       "      <td>2214.41</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>2037.49</td>\n",
       "      <td>2517.99</td>\n",
       "      <td>1946.46</td>\n",
       "      <td>2223.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>2029.00</td>\n",
       "      <td>2515.96</td>\n",
       "      <td>1939.80</td>\n",
       "      <td>2221.47</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>2034.10</td>\n",
       "      <td>2535.10</td>\n",
       "      <td>1942.71</td>\n",
       "      <td>2229.83</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LV       LG       SV       SG  Mkt-RF   SMB   HML    RF\n",
       "DATE                                                                    \n",
       "2020-01-02  2050.79  2517.44  1955.12  2218.42    0.86 -0.88 -0.34  0.01\n",
       "2020-01-03  2035.21  2502.53  1948.33  2214.41   -0.67  0.38  0.01  0.01\n",
       "2020-01-06  2037.49  2517.99  1946.46  2223.60    0.36 -0.07 -0.55  0.01\n",
       "2020-01-07  2029.00  2515.96  1939.80  2221.47   -0.19 -0.01 -0.26  0.01\n",
       "2020-01-08  2034.10  2535.10  1942.71  2229.83    0.47 -0.07 -0.64  0.01"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "crsp = pd.read_csv(\"CRSP_Indexes.csv\", parse_dates=['DATE'])\n",
    "ff   = pd.read_csv(\"FamaFrenchDaily.csv\", parse_dates=['Date'])\n",
    "\n",
    "# Merge data at indexes where only CRSP has data\n",
    "Merge_df = crsp.merge(ff, how='left', left_on='DATE', right_on='Date')\n",
    "Merge_df.set_index('DATE', inplace=True)\n",
    "Merge_df.drop(columns=['Date'], inplace=True)\n",
    "Merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42a674439017879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:24.939133Z",
     "start_time": "2025-11-22T06:04:24.910353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annualized Mean Excess Return</th>\n",
       "      <th>Annualized Volatility</th>\n",
       "      <th>Annualized Sharpe Ratio</th>\n",
       "      <th>T-Statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LV</th>\n",
       "      <td>0.064487</td>\n",
       "      <td>0.200205</td>\n",
       "      <td>0.322106</td>\n",
       "      <td>0.753496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LG</th>\n",
       "      <td>0.165763</td>\n",
       "      <td>0.259616</td>\n",
       "      <td>0.638494</td>\n",
       "      <td>1.493615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SV</th>\n",
       "      <td>0.074011</td>\n",
       "      <td>0.264932</td>\n",
       "      <td>0.279358</td>\n",
       "      <td>0.653495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG</th>\n",
       "      <td>0.070021</td>\n",
       "      <td>0.272320</td>\n",
       "      <td>0.257128</td>\n",
       "      <td>0.601495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Annualized Mean Excess Return  Annualized Volatility  \\\n",
       "LV                       0.064487               0.200205   \n",
       "LG                       0.165763               0.259616   \n",
       "SV                       0.074011               0.264932   \n",
       "SG                       0.070021               0.272320   \n",
       "\n",
       "    Annualized Sharpe Ratio  T-Statistic  \n",
       "LV                 0.322106     0.753496  \n",
       "LG                 0.638494     1.493615  \n",
       "SV                 0.279358     0.653495  \n",
       "SG                 0.257128     0.601495  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate excess returns for each ETFs\n",
    "etf_list = ['LV', 'LG', 'SV', 'SG']\n",
    "Merge_df['rf'] = Merge_df['RF'] / 100.0\n",
    "\n",
    "for etf in etf_list:\n",
    "    Merge_df[f'{etf}_Return'] = Merge_df[etf].pct_change()\n",
    "    Merge_df[f'{etf}_Excess_Return'] = Merge_df[f'{etf}_Return'] - Merge_df['rf']\n",
    "# Calculate the annual mean excess return, annual volatility, annual Sharpe ratio, and t-statistic for each ETF\n",
    "results = {}\n",
    "trading_days = 252\n",
    "for etf in etf_list:\n",
    "    er = Merge_df[f'{etf}_Excess_Return']      # Compute excess returns\n",
    "    er = er.dropna()                           # Remove NaN values\n",
    "    N  = er.count()\n",
    "    mean_daily = er.mean()\n",
    "    std_daily  = er.std(ddof=1)\n",
    "\n",
    "    ann_mean = mean_daily * trading_days\n",
    "    ann_std  = std_daily * np.sqrt(trading_days)\n",
    "    sharpe   = ann_mean / ann_std\n",
    "    t_stat   = mean_daily / (std_daily / np.sqrt(N))\n",
    "    results[etf] = {\n",
    "        'Annualized Mean Excess Return': ann_mean,\n",
    "        'Annualized Volatility': ann_std,\n",
    "        'Annualized Sharpe Ratio': sharpe,\n",
    "        'T-Statistic': t_stat\n",
    "    }\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b3a290312244a",
   "metadata": {},
   "source": [
    "### Question 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd96f5488498c26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:25.260693Z",
     "start_time": "2025-11-22T06:04:25.234639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annualized Mean Excess Return</th>\n",
       "      <th>Annualized Volatility</th>\n",
       "      <th>Annualized Sharpe Ratio</th>\n",
       "      <th>T-Statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25%_Equal_Weighted_Portfolio</th>\n",
       "      <td>0.093571</td>\n",
       "      <td>0.233336</td>\n",
       "      <td>0.401012</td>\n",
       "      <td>0.938079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Annualized Mean Excess Return  \\\n",
       "25%_Equal_Weighted_Portfolio                       0.093571   \n",
       "\n",
       "                              Annualized Volatility  Annualized Sharpe Ratio  \\\n",
       "25%_Equal_Weighted_Portfolio               0.233336                 0.401012   \n",
       "\n",
       "                              T-Statistic  \n",
       "25%_Equal_Weighted_Portfolio     0.938079  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the portfolio return as the average return of the four ETFs, equally weighted\n",
    "Merge_df['Port_Return'] = Merge_df[[f'{etf}_Return' for etf in etf_list]].mean(axis=1)\n",
    "Merge_df['Port_Excess_Return'] = Merge_df['Port_Return'] - Merge_df['rf']\n",
    "\n",
    "er_p = Merge_df['Port_Excess_Return'].dropna()\n",
    "N_p  = er_p.count()\n",
    "mean_daily_p = er_p.mean()\n",
    "std_daily_p  = er_p.std(ddof=1)\n",
    "\n",
    "ann_mean_p = mean_daily_p * trading_days\n",
    "ann_std_p  = std_daily_p * np.sqrt(trading_days)\n",
    "sharpe_p   = ann_mean_p / ann_std_p\n",
    "t_stat_p   = mean_daily_p / (std_daily_p / np.sqrt(N_p))\n",
    "\n",
    "results_port = {\n",
    "    'Annualized Mean Excess Return': ann_mean_p,\n",
    "    'Annualized Volatility': ann_std_p,\n",
    "    'Annualized Sharpe Ratio': sharpe_p,\n",
    "    'T-Statistic': t_stat_p\n",
    "}\n",
    "\n",
    "results_port_df = pd.DataFrame(results_port, index=['25%_Equal_Weighted_Portfolio'])\n",
    "results_port_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad80868",
   "metadata": {},
   "source": [
    "### Question 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf081c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:25.696673Z",
     "start_time": "2025-11-22T06:04:25.668326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annualized Mean Excess Return</th>\n",
       "      <th>Annualized Volatility</th>\n",
       "      <th>Annualized Sharpe Ratio</th>\n",
       "      <th>T-Statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LV_Timing</th>\n",
       "      <td>0.015066</td>\n",
       "      <td>0.102326</td>\n",
       "      <td>0.147239</td>\n",
       "      <td>0.344432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LG_Timing</th>\n",
       "      <td>0.127856</td>\n",
       "      <td>0.139285</td>\n",
       "      <td>0.917942</td>\n",
       "      <td>2.147322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SV_Timing</th>\n",
       "      <td>0.043846</td>\n",
       "      <td>0.136948</td>\n",
       "      <td>0.320161</td>\n",
       "      <td>0.748946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG_Timing</th>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.140464</td>\n",
       "      <td>0.106037</td>\n",
       "      <td>0.248051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Annualized Mean Excess Return  Annualized Volatility  \\\n",
       "LV_Timing                       0.015066               0.102326   \n",
       "LG_Timing                       0.127856               0.139285   \n",
       "SV_Timing                       0.043846               0.136948   \n",
       "SG_Timing                       0.014894               0.140464   \n",
       "\n",
       "           Annualized Sharpe Ratio  T-Statistic  \n",
       "LV_Timing                 0.147239     0.344432  \n",
       "LG_Timing                 0.917942     2.147322  \n",
       "SV_Timing                 0.320161     0.748946  \n",
       "SG_Timing                 0.106037     0.248051  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement a timing strategy based on 200-day moving average for each ETF\n",
    "window = 200\n",
    "timing_results = {}\n",
    "\n",
    "for etf in etf_list:\n",
    "    price = Merge_df[etf]\n",
    "\n",
    "    # 200-day moving average of price, using only past information\n",
    "    ma_200 = price.rolling(window=window).mean().shift(1)   # 200-day MA up to yesterday\n",
    "    lag_price = price.shift(1)                              # yesterday's price\n",
    "\n",
    "    # Signal: 1 if yesterday's price > yesterday's 200d MA, else 0\n",
    "    signal = (lag_price > ma_200).astype(int)\n",
    "\n",
    "    # Timing strategy excess return:\n",
    "    # when signal==1 we hold the ETF, otherwise we hold cash (rf).\n",
    "    # Excess return vs rf is therefore signal * ETF_excess_return.\n",
    "    col_name = f'{etf}_Timing_Excess_Return'\n",
    "    Merge_df[col_name] = signal * Merge_df[f'{etf}_Excess_Return']\n",
    "\n",
    "    # Drop NaNs from the warm-up period (first 200 days, etc.)\n",
    "    er_t = Merge_df[col_name].dropna()\n",
    "    N_t = er_t.count()\n",
    "\n",
    "    mean_daily_t = er_t.mean()\n",
    "    std_daily_t  = er_t.std(ddof=1)\n",
    "\n",
    "    # Annualize\n",
    "    ann_mean_t = mean_daily_t * trading_days\n",
    "    ann_std_t  = std_daily_t * np.sqrt(trading_days)\n",
    "    sharpe_t   = ann_mean_t / ann_std_t\n",
    "    t_stat_t   = mean_daily_t / (std_daily_t / np.sqrt(N_t))\n",
    "\n",
    "    timing_results[f'{etf}_Timing'] = {\n",
    "        'Annualized Mean Excess Return': ann_mean_t,\n",
    "        'Annualized Volatility': ann_std_t,\n",
    "        'Annualized Sharpe Ratio': sharpe_t,\n",
    "        'T-Statistic': t_stat_t\n",
    "    }\n",
    "\n",
    "# Collect results into a DataFrame\n",
    "timing_results_df = pd.DataFrame(timing_results).T\n",
    "timing_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36af74d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:26.055610Z",
     "start_time": "2025-11-22T06:04:26.050747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ETF for the 200-day MA timing strategy (by Sharpe Ratio): LG_Timing\n"
     ]
    }
   ],
   "source": [
    "best_etf = timing_results_df['Annualized Sharpe Ratio'].idxmax()\n",
    "print(\"Best ETF for the 200-day MA timing strategy (by Sharpe Ratio):\", best_etf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a927b8",
   "metadata": {},
   "source": [
    "Based on the result, the best ETF for the 200-day MA timing strategy in terms of Sharpe Ratio is the Large Cap Growth ETF (LG_Timing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29671b4b026730be",
   "metadata": {},
   "source": [
    "## <mark> Question solution summary: <mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95918fc32f6d81",
   "metadata": {},
   "source": [
    "In **Question 1**, we analyze the performance and risk characteristics of four style-based equity ETFs and assess whether a simple technical timing rule can improve their returns. In (a), we compute daily excess returns for the Large Value, Large Growth, Small Value, and Small Growth ETFs, and evaluate each using annualized mean excess return, volatility, Sharpe ratio, and t-statistics. In (b), we form an equal-weighted portfolio of the four ETFs and calculate its excess returns and performance metrics to assess the benefits of diversification. In (c), we apply a 200-day moving-average timing strategy to each ETF and compare the resulting risk-adjusted performance to the corresponding buy-and-hold results. Overall, Large Growth exhibits the strongest buy-and-hold performance, the equal-weighted portfolio provides moderate diversification benefits, and the moving-average rule enhances performance mainly for Large Growth while offering limited improvement for the other ETFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c01ea",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8537b9",
   "metadata": {},
   "source": [
    "### Question 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f79ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:40.375284Z",
     "start_time": "2025-11-22T06:04:26.182791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ret   Rfree        dp     tms     dfy      ntis        ER\n",
      "date                                                                      \n",
      "1950-01-01  0.019703  0.0009  0.067449  0.0108  0.0067  0.027102  0.018803\n",
      "1950-02-01  0.019603  0.0009  0.067364  0.0102  0.0066  0.025492  0.018703\n",
      "1950-03-01  0.008185  0.0010  0.067669  0.0103  0.0066  0.029291  0.007185\n",
      "1950-04-01  0.045887  0.0009  0.065302  0.0099  0.0063  0.026398  0.044987\n",
      "1950-05-01  0.046902  0.0010  0.063365  0.0097  0.0064  0.028572  0.045902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "\n",
    "# Load data and prepare\n",
    "def load_and_prepare_data():\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel('PredictorData2024.xlsx', sheet_name='Monthly')\n",
    "    \n",
    "    # Convert yyyymm to date format\n",
    "    df['date'] = pd.to_datetime(df['yyyymm'].astype(str), format='%Y%m')\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Keep data from 1950-01-01 onwards\n",
    "    df = df[df.index >= '1950-01-01']\n",
    "    \n",
    "    # Keep only required variables and rename\n",
    "    keep_cols = ['ret', 'Rfree', 'd/p', 'tms', 'dfy', 'ntis']\n",
    "    df = df[keep_cols].copy()\n",
    "    df.rename(columns={'d/p': 'dp'}, inplace=True)\n",
    "    \n",
    "    # Calculate excess return\n",
    "    df['ER'] = df['ret'] - df['Rfree']\n",
    "    \n",
    "    # Print results\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94a73e",
   "metadata": {},
   "source": [
    "### Question 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3c3adb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:44.069705Z",
     "start_time": "2025-11-22T06:04:40.534539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ret   Rfree        dp     tms     dfy      ntis        ER\n",
      "date                                                                      \n",
      "1950-01-01  0.019703  0.0009  0.067449  0.0108  0.0067  0.027102  0.018803\n",
      "1950-02-01  0.019603  0.0009  0.067364  0.0102  0.0066  0.025492  0.018703\n",
      "1950-03-01  0.008185  0.0010  0.067669  0.0103  0.0066  0.029291  0.007185\n",
      "1950-04-01  0.045887  0.0009  0.065302  0.0099  0.0063  0.026398  0.044987\n",
      "1950-05-01  0.046902  0.0010  0.063365  0.0097  0.0064  0.028572  0.045902\n",
      "Data period: 1950-01-01 00:00:00 to 2024-12-01 00:00:00\n",
      "Total observations: 900\n",
      "Prediction period: 1960-01-01 00:00:00 to 2024-12-01 00:00:00\n",
      "Total predictions: 780\n",
      "First prediction value (1960-01): 0.000797\n",
      "\n",
      "First 10 predictions:\n",
      "            predicted_ER\n",
      "1960-01-01      0.000797\n",
      "1960-02-01     -0.001679\n",
      "1960-03-01      0.001897\n",
      "1960-04-01      0.006327\n",
      "1960-05-01      0.009289\n",
      "1960-06-01      0.009219\n",
      "1960-07-01      0.019308\n",
      "1960-08-01      0.015500\n",
      "1960-09-01      0.017106\n",
      "1960-10-01      0.009584\n",
      "\n",
      "Prediction statistics summary:\n",
      "       predicted_ER\n",
      "count    780.000000\n",
      "mean       0.005197\n",
      "std        0.013661\n",
      "min       -0.063004\n",
      "25%       -0.004632\n",
      "50%        0.005038\n",
      "75%        0.014164\n",
      "max        0.050763\n"
     ]
    }
   ],
   "source": [
    "# Execute rolling window regression predictions\n",
    "def rolling_regression_predictions(df):\n",
    "    \"\"\"\n",
    "    Use 10-year rolling window for regression predictions.\n",
    "    The first prediction is for January 1960, based on data from\n",
    "    Jan 1950 - Dec 1959.\n",
    "    \"\"\"\n",
    "    # Create lagged variables\n",
    "    df['dp_lag1'] = df['dp'].shift(1)\n",
    "    df['tms_lag1'] = df['tms'].shift(1)\n",
    "    df['dfy_lag1'] = df['dfy'].shift(1)\n",
    "    df['ntis_lag1'] = df['ntis'].shift(1)\n",
    "\n",
    "    # Store prediction results\n",
    "    predictions = []\n",
    "    prediction_dates = []\n",
    "\n",
    "    # Set window size (10 years = 120 months)\n",
    "    window_size = 120\n",
    "\n",
    "    # Rolling window regression\n",
    "    # i = window_size → 用 df.iloc[0:120]（1950-01 ~ 1959-12）预测第 120 条（1960-01）\n",
    "    for i in range(window_size, len(df)):\n",
    "        # Current prediction date (the month we are forecasting)\n",
    "        current_date = df.index[i]\n",
    "\n",
    "        # Extract window data (past 10 years)\n",
    "        window_data = df.iloc[i-window_size:i]\n",
    "\n",
    "        # Remove rows with NaN (due to lagged variables)\n",
    "        # 理想情况下应该剩下 119 条观测（10 年减去第一个 lag 丢掉的一条）\n",
    "        window_data_clean = window_data.dropna(subset=['ER', 'dp_lag1', 'tms_lag1', 'dfy_lag1', 'ntis_lag1'])\n",
    "\n",
    "        # Ensure we have exactly 119 observations in the regression window\n",
    "        if len(window_data_clean) < window_size - 1:\n",
    "            continue\n",
    "\n",
    "        # Prepare regression data\n",
    "        X = window_data_clean[['dp_lag1', 'tms_lag1', 'dfy_lag1', 'ntis_lag1']]\n",
    "        y = window_data_clean['ER']\n",
    "\n",
    "        # Add constant term\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        try:\n",
    "            # Run OLS regression\n",
    "            model = sm.OLS(y, X).fit()\n",
    "\n",
    "            # Prepare predictors for forecasting:\n",
    "            # use the previous month's actual predictors as input (t-1)\n",
    "            last_obs = df.iloc[i-1]\n",
    "            X_pred = pd.DataFrame({\n",
    "                'const': [1],\n",
    "                'dp_lag1': [last_obs['dp']],\n",
    "                'tms_lag1': [last_obs['tms']],\n",
    "                'dfy_lag1': [last_obs['dfy']],\n",
    "                'ntis_lag1': [last_obs['ntis']]\n",
    "            })\n",
    "\n",
    "            # Make prediction of ER_t for current_date\n",
    "            pred = model.predict(X_pred)[0]\n",
    "\n",
    "            # Store results\n",
    "            predictions.append(pred)\n",
    "            prediction_dates.append(current_date)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Regression failed at {current_date}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create prediction results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'predicted_ER': predictions\n",
    "    }, index=prediction_dates)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    df = load_and_prepare_data()\n",
    "    print(f\"Data period: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Total observations: {len(df)}\")\n",
    "\n",
    "    predictions = rolling_regression_predictions(df)\n",
    "\n",
    "    print(f\"Prediction period: {predictions.index.min()} to {predictions.index.max()}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"First prediction value ({predictions.index[0].strftime('%Y-%m')}): \"\n",
    "          f\"{predictions.iloc[0]['predicted_ER']:.6f}\")\n",
    "\n",
    "    # Display first few predictions\n",
    "    print(\"\\nFirst 10 predictions:\")\n",
    "    print(predictions.head(10))\n",
    "\n",
    "    # Display statistical summary\n",
    "    print(\"\\nPrediction statistics summary:\")\n",
    "    print(predictions.describe())\n",
    "\n",
    "    return df, predictions\n",
    "\n",
    "# Execute main program\n",
    "df, predictions = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb5110",
   "metadata": {},
   "source": [
    "### Question 2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8327ff66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T06:04:44.141541Z",
     "start_time": "2025-11-22T06:04:44.115943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Question 2.c: Timing Strategy vs Buy-and-Hold =====\n",
      "                 Annualized Mean Excess Return  Annualized Volatility  \\\n",
      "Timing_Strategy                       0.077941               0.140509   \n",
      "Buy_and_Hold                          0.069185               0.149409   \n",
      "\n",
      "                 Annualized Sharpe Ratio  T-Statistic  \n",
      "Timing_Strategy                 0.554703     4.472161  \n",
      "Buy_and_Hold                    0.463057     3.733288  \n"
     ]
    }
   ],
   "source": [
    "def perf_stats(r, freq=12):\n",
    "    \"\"\"\n",
    "    Compute performance statistics for a monthly excess return series.\n",
    "    Output includes annualized mean, annualized volatility, Sharpe ratio,\n",
    "    and the t-statistic of the mean.\n",
    "    \n",
    "    Parameters:\n",
    "        r (Series): Monthly excess returns\n",
    "        freq (int): Annualization factor (12 for monthly data)\n",
    "    \"\"\"\n",
    "    r = r.dropna()\n",
    "    N = len(r)\n",
    "    mean_m = r.mean()\n",
    "    std_m = r.std(ddof=1)\n",
    "\n",
    "    ann_mean = mean_m * freq\n",
    "    ann_std = std_m * np.sqrt(freq)\n",
    "    sharpe = ann_mean / ann_std\n",
    "    t_stat = mean_m / (std_m / np.sqrt(N))\n",
    "\n",
    "    return {\n",
    "        'Annualized Mean Excess Return': ann_mean,\n",
    "        'Annualized Volatility': ann_std,\n",
    "        'Annualized Sharpe Ratio': sharpe,\n",
    "        'T-Statistic': t_stat\n",
    "    }\n",
    "\n",
    "\n",
    "def timing_strategy(df, predictions):\n",
    "    \"\"\"\n",
    "    Construct the timing strategy using predicted excess returns.\n",
    "    Weight rule:\n",
    "        w_t = min(1.5, max(0.5, 100 * ER_hat_t))\n",
    "\n",
    "    Compare:\n",
    "        - Timing strategy (w_t * ER_t)\n",
    "        - Buy-and-hold (ER_t)\n",
    "\n",
    "    Returns:\n",
    "        new_df      DataFrame with ER, predicted_ER, w_t, Timing_ER, BH_ER\n",
    "        results_df  Performance stats for both strategies\n",
    "    \"\"\"\n",
    "    # Merge actual ER and predicted ER\n",
    "    new_df = df[['ER']].join(predictions[['predicted_ER']], how='inner')\n",
    "\n",
    "    # Remove NaN rows (due to 10-year rolling window lags)\n",
    "    new_df = new_df.dropna(subset=['ER', 'predicted_ER'])\n",
    "\n",
    "    # Compute portfolio weight w_t = min(1.5, max(0.5, 100 × ER_hat_t))\n",
    "    new_df['w_t'] = 100.0 * new_df['predicted_ER']\n",
    "    new_df['w_t'] = new_df['w_t'].clip(lower=0.5, upper=1.5)\n",
    "\n",
    "    # Timing strategy excess return\n",
    "    new_df['Timing_ER'] = new_df['w_t'] * new_df['ER']\n",
    "\n",
    "    # Buy-and-hold strategy excess return\n",
    "    new_df['BH_ER'] = new_df['ER']\n",
    "\n",
    "    # Compute performance statistics\n",
    "    timing_stats = perf_stats(new_df['Timing_ER'])\n",
    "    bh_stats = perf_stats(new_df['BH_ER'])\n",
    "\n",
    "    # Create formatted comparison table\n",
    "    results_df = pd.DataFrame({\n",
    "        'Timing_Strategy': timing_stats,\n",
    "        'Buy_and_Hold': bh_stats\n",
    "    }).T\n",
    "\n",
    "    print(\"\\n===== Question 2.c: Timing Strategy vs Buy-and-Hold =====\")\n",
    "    print(results_df)\n",
    "\n",
    "    return new_df, results_df\n",
    "\n",
    "def main():\n",
    "\n",
    "    new_df, results_df = timing_strategy(df, predictions)\n",
    "    # return results\n",
    "    return new_df, results_df\n",
    "\n",
    "# Execute main program\n",
    "new_df, results_df = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9ad25",
   "metadata": {},
   "source": [
    "Based on the results, the timing strategy outperforms the buy-and-hold benchmark. It delivers a higher annualized mean excess return (7.79% vs. 6.92%) while also slightly reducing volatility (14.05% vs. 14.94%). Consequently, the Sharpe ratio improves from 0.46 for the buy-and-hold strategy to 0.55 for the timing strategy, and the t-statistic of the mean excess return rises from 3.73 to 4.47. Taken together, these results indicate that, over this sample period, an investor would have achieved higher and more statistically significant risk-adjusted performance by using the predictor-based timing rule rather than simply holding the market portfolio. This suggests that the timing strategy effectively capitalizes on the predictive power of the selected variables to enhance returns while managing risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9d24a210ebe23",
   "metadata": {},
   "source": [
    "## <mark> Question solution summary: <mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6321d",
   "metadata": {},
   "source": [
    "In **Question 2**, we examine whether common predictive variables can forecast monthly excess market returns and whether these forecasts improve portfolio performance. In (a), we load and clean the \"PredictorData2024\" dataset, construct the excess market return series, and retain the four predictors (dp, tms, dfy, ntis). In (b), we implement a 10-year rolling predictive regression, generating a time series of out-of-sample forecasts for excess returns starting in 1960. In (c), we use these forecasts to build a dynamic timing strategy with weights constrained between 0.5 and 1.5. Comparing this strategy with a passive buy-and-hold benchmark, we find that the timing strategy achieves higher mean excess returns, lower volatility, and a significantly higher Sharpe ratio. Overall, the predictors provide meaningful forecasting power, and using them for market timing improves performance relative to simply holding the market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
